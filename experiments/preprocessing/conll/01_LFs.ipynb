{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "\n",
    "from allennlp.data import Token\n",
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.dataset_readers.conll2003 import Conll2003DatasetReader\n",
    "import pandas as pd\n",
    "from allennlp.common.params import Params\n",
    "from wiser.lf import LabelingFunction, LinkingFunction, DictionaryMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14041it [00:01, 13459.07it/s]\n",
      "3250it [00:00, 10188.35it/s]\n",
      "3453it [00:00, 14033.52it/s]\n"
     ]
    }
   ],
   "source": [
    "root_directory = '../../..'\n",
    "\n",
    "reader = Conll2003DatasetReader(coding_scheme=\"BIOUL\")\n",
    "train_data = reader.read(root_directory + '/data/conll/eng.train')\n",
    "dev_data = reader.read(root_directory + '/data/conll/eng.testa')\n",
    "test_data = reader.read(root_directory + '/data/conll/eng.testb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_docs = train_data + dev_data + test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses Spacy to add NLP information to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_docs = []\n",
    "for doc in conll_docs:\n",
    "    words = [token.text for token in doc['tokens']]\n",
    "    spaces = [True] * (len(words) - 1)\n",
    "    spaces.append(False)\n",
    "    spacy_doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "    for name, proc in nlp.pipeline:\n",
    "        spacy_doc = proc(spacy_doc)\n",
    "    \n",
    "    tokens = [Token(token.text,\n",
    "                    token.idx,\n",
    "                    token.lemma_,\n",
    "                    token.pos_,\n",
    "                    token.tag_,\n",
    "                    token.dep_,\n",
    "                    token.ent_type_) for token in spacy_doc]\n",
    "    \n",
    "    doc.add_field('tokens', TextField(tokens, doc['tokens']._token_indexers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads dictionaries from DBpedia and other terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = set()\n",
    "with open('entity_data/people.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            people.add(line.strip())\n",
    "\n",
    "countries = set()\n",
    "with open('entity_data/countries.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            countries.add(line.strip())\n",
    "\n",
    "pop_places = set()\n",
    "with open('entity_data/populated_places.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            pop_places.add(line.strip())\n",
    "\n",
    "orgs = set()\n",
    "with open('entity_data/organisations.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            orgs.add(line.strip())\n",
    "\n",
    "yago_orgs = set()\n",
    "with open('entity_data/yago_orgs.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            yago_orgs.add(line.strip())\n",
    "\n",
    "yago_companies = set()\n",
    "with open('entity_data/yago_companies.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            yago_companies.add(line.strip())\n",
    "\n",
    "demonyms = set()\n",
    "with open('entity_data/demonyms.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            demonyms.add(line.strip())\n",
    "            \n",
    "days_of_the_week = {\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\",\n",
    "                    \"Thursday\", \"Friday\", \"Saturday\"}\n",
    "\n",
    "months_of_the_year = {\"January\", \"February\", \"March\", \"April\",\n",
    "                      \"May\", \"June\", \"July\", \"August\", \"September\",\n",
    "                      \"October\", \"November\", \"December\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions - PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_terms = [person.split(\" \") for person in people]\n",
    "lf = DictionaryMatcher(\"DBpediaPeople\", people_terms, i_label=\"I-PER\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_name_counts = {}\n",
    "for name in people:\n",
    "    name = name.split(\" \")\n",
    "    if len(name) == 2:\n",
    "        if name[0] in first_name_counts:\n",
    "            first_name_counts[name[0]] += 1\n",
    "        else:\n",
    "            first_name_counts[name[0]] = 1\n",
    "            \n",
    "first_names = set()\n",
    "for first_name, count in first_name_counts.items():\n",
    "    if count > 10:\n",
    "        first_names.add(first_name)\n",
    "\n",
    "first_names -= countries\n",
    "first_names -= pop_places\n",
    "first_names -= days_of_the_week\n",
    "first_names -= months_of_the_year\n",
    "first_names -= demonyms\n",
    "first_names -= {\"The\", \"He\", \"East\", \"West\", \"North\", \"South\"}\n",
    "\n",
    "lf = DictionaryMatcher(\"FirstNames\", [[x] for x in first_names], i_label=\"I-PER\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_name_counts = {}\n",
    "for name in people:\n",
    "    name = name.split(\" \")\n",
    "    if len(name) == 2:\n",
    "        if name[1] in second_name_counts:\n",
    "            second_name_counts[name[1]] += 1\n",
    "        else:\n",
    "            second_name_counts[name[1]] = 1\n",
    "            \n",
    "second_names = set()\n",
    "for second_name, count in second_name_counts.items():\n",
    "    if count > 2 and len(second_name) > 3:\n",
    "        second_names.add(second_name)\n",
    "        \n",
    "second_names -= countries\n",
    "second_names -= pop_places\n",
    "second_names -= days_of_the_week\n",
    "second_names -= months_of_the_year\n",
    "second_names -= demonyms\n",
    "second_names -= {\"The\", \"He\", \"East\", \"West\", \"North\", \"South\"}\n",
    "\n",
    "lf = DictionaryMatcher(\"SecondNames\", [[x] for x in second_names], i_label=\"I-PER\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Said(LabelingFunction):\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        text = [token.text for token in instance['tokens']]\n",
    "        \n",
    "        for i in range (1, len(text) - 1):\n",
    "            if text[i] == \"said\" and text[i-1][0].isupper():\n",
    "                labels[i-1] = 'I-PER'\n",
    "            elif text[i] == \"said\" and text[i+1][0].isupper():\n",
    "                labels[i+1] = 'I-PER'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = Said()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions - LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_terms = [country.lower().split(\" \") for country in countries]\n",
    "lf = DictionaryMatcher(\"DBpediaCountries\", country_terms,\n",
    "                       i_label=\"I-LOC\", match_lemmas=True)\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_place_terms = [pop_place.lower().split(\" \") for pop_place in pop_places]\n",
    "lf = DictionaryMatcher(\"DBpediaPopPlaces\", pop_place_terms,\n",
    "                       i_label=\"I-LOC\", match_lemmas=True)\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrv_terms = [\n",
    "    [\"U.S.\"], [\"US\"], [\"U.S.A.\"], [\"USA\"], [\"UAE\"]\n",
    "]\n",
    "lf = DictionaryMatcher(\"CountryAbbrvs\", abbrv_terms, i_label=\"I-LOC\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions - ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_terms = [org.split(\" \") for org in orgs]\n",
    "lf = DictionaryMatcher(\"DBpediaORG\", org_terms, i_label=\"I-ORG\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yago_org_terms = [yago_org.split(\" \") for yago_org in yago_orgs]\n",
    "lf = DictionaryMatcher(\"YAGO_ORG\", yago_org_terms, i_label=\"I-ORG\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yago_company_terms = [\n",
    "    yago_company.split(\" \") for yago_company in yago_companies\n",
    "]\n",
    "lf = DictionaryMatcher(\"YAGO_Companies\", yago_company_terms, i_label=\"I-ORG\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrgLastWord(LabelingFunction):\n",
    "\n",
    "    org_last = {\n",
    "        'Observatory', 'Exchange', 'University', 'Co', 'Ltd', 'Airport',\n",
    "        'Inc', 'Inc.', 'Corp', 'Corp.', 'School', 'Enterprise', 'Education', 'Research', \n",
    "        'Development', 'Heritage', 'Technology', 'Infrastructure', 'Networks', \n",
    "        'Chambers', 'Academy', 'Hotels', 'Fund', 'Studios', 'Agency', \n",
    "        'Bureau', 'Treasury', 'Newsroom', 'Federation', 'League', 'Club', 'Group', 'Force', \n",
    "        'Department', 'Administration', 'Campaign', 'Authority', 'Party', \n",
    "        'Organisation', 'Council', 'Community', 'Newsroom', 'Desk', 'Management', \n",
    "        'Association', 'Reserve', 'Committee', 'Power', 'Bank', 'Services', 'Copper', \n",
    "        'Managers', 'Newsdesk', 'Radio', 'Authority', 'Commerce', 'Cables', 'Group', \n",
    "        'Labour', 'Office', 'Congress', 'Corporation', 'Front', 'Court', 'Community', \n",
    "        'Software', 'Pizza', 'Cargo', 'Books', 'Casinos', 'Bar', 'College', \n",
    "        'Commando', 'Force', 'Army', 'District', 'Movies', 'Center', \n",
    "        'Entertainment', 'Channels', 'Interactive', 'Systems', 'Pictures', \n",
    "        'Talk', 'Media', 'Stations', 'Park', 'Arts', 'Office', 'Analytics', \n",
    "        'Production', 'Basketball', 'Restaurant', 'Center', 'Pictures', \n",
    "        'Brewery', 'Institute', 'Partners', 'Forum', 'Foundation', \n",
    "        'Crafts', 'Nursery', 'Publications', 'Cars', 'Society', 'Sciences'}\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i].text in self.org_last:\n",
    "                if instance['tokens'][i-1].text[0].isupper():\n",
    "                    labels[i-1] = 'I-ORG'\n",
    "                labels[i] = 'I-ORG'\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "lf = OrgLastWord()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions - MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_terms = {\"Democrat\", \"Republican\", \"Liberal\", \"Conservative\", \"Arab\",\n",
    "                    \"Christian\", \"Muslim\", \"Jewish\", \"Buddhist\", \"Hindu\"}\n",
    "temp = demonyms | additional_terms\n",
    "lf = DictionaryMatcher(\"DBPediaMISC\", [term.lower().split(\" \") for term in temp if len(term) > 2],\n",
    "                       match_lemmas=True, i_label=\"I-MISC\")\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiscLastWord(LabelingFunction):\n",
    "\n",
    "    misc_last = {'Cup', 'Open', 'Championship', 'Festival', 'League', 'Tour', 'Tournament', \n",
    "                 'War', 'Revolution', 'Act', 'Treaty', 'Symposium', 'Day', 'Series', \n",
    "                 'Game', 'Central', 'Network', 'Division', 'Baseball', 'Enterprise', 'Protocol'}\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i].text in self.misc_last:\n",
    "                labels[i-1] = 'I-MISC'\n",
    "                labels[i] = 'I-MISC'\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "lf = MiscLastWord()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiscAdj(LabelingFunction):\n",
    "    misc_adj ={'based', 'bound', 'born', 'ruled', 'backed', 'listed'}\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [token.text for token in instance['tokens']]\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].split(\"-\")[-1].lower() in self.misc_adj:\n",
    "                if tokens[i][0].isupper():\n",
    "                    labels[i] = 'I-MISC'\n",
    "                else:\n",
    "                    labels[i] = 'O'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = MiscAdj()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions - O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerbOrAdv(LabelingFunction):\n",
    "    pos = {\"VERB\", \"ADV\"}\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i, pos in enumerate([token.pos_ for token in instance['tokens']]):\n",
    "            if pos in self.pos:\n",
    "                labels[i] = 'O'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = VerbOrAdv()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuation(LabelingFunction):\n",
    "    pos = {\"PUNCT\"}\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i, pos in enumerate([token.pos_ for token in instance['tokens']]):\n",
    "            if pos in self.pos:\n",
    "                labels[i] = 'O'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = Punctuation()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pronouns(LabelingFunction):\n",
    "    pos = {\"PRON\"}\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i, pos in enumerate([token.pos_ for token in instance['tokens']]):\n",
    "            if pos in self.pos:\n",
    "                labels[i] = 'O'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = Pronouns()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Numbers(LabelingFunction):\n",
    "    pos = {\"NUM\"}\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i, pos in enumerate([token.pos_ for token in instance['tokens']]):\n",
    "            if pos in self.pos:\n",
    "                labels[i] = 'O'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = Numbers()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongLowerCase(LabelingFunction):\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        \n",
    "        for i, text in enumerate([token.text for token in instance['tokens']]):\n",
    "            if not text[0].isupper() and len(text) > 4:\n",
    "                labels[i] = 'O'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = LongLowerCase()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveLowerCase(LabelingFunction):\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "        text = [token.text for token in instance['tokens']]\n",
    "        \n",
    "        for i in range (1, len(text) - 1):\n",
    "            if not text[i-1][0].isupper() and not text[i][0].isupper() and not text[i+1][0].isupper():\n",
    "                labels[i] = 'O'\n",
    "        \n",
    "        return labels\n",
    "\n",
    "lf = ConsecutiveLowerCase()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiser.lf import ElmoLinkingFunction\n",
    "\n",
    "lf = ElmoLinkingFunction(.75)\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundPhrase(LinkingFunction):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        for i in range(1, len(instance['tokens'])):\n",
    "            if instance['tokens'][i-1].dep_ == \"compound\":\n",
    "                links[i] = 1\n",
    "        \n",
    "        return links\n",
    "\n",
    "lf = CompoundPhrase()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveCapitals(LinkingFunction):\n",
    "    def apply_instance(self, instance):\n",
    "        links = [0] * len(instance['tokens'])\n",
    "        # We skip the first pair since the first\n",
    "        # token is almost always capitalized\n",
    "        for i in range(2, len(instance['tokens'])):\n",
    "            # We skip this token if it all capitals\n",
    "            all_caps = True\n",
    "            text = instance['tokens'][i].text\n",
    "            for char in text:\n",
    "                if char.islower():\n",
    "                    all_caps = False\n",
    "                    break\n",
    "            \n",
    "            if not all_caps and text[0].isupper() \\\n",
    "            and instance['tokens'][i-1].text[0].isupper():\n",
    "                links[i] = 1\n",
    "        \n",
    "        return links\n",
    "\n",
    "lf = ConsecutiveCapitals()\n",
    "lf.apply(conll_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves Weak Supervision to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tmp/train_data.p', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open('tmp/dev_data.p', 'wb') as f:\n",
    "    pickle.dump(dev_data, f)\n",
    "    \n",
    "with open('tmp/test_data.p', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Part 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
